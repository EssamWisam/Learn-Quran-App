{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read all the SurahTopics as manually extracted from Shamela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# read Topics.md as a list of strings\n",
    "with open('surahTopics.md') as f:\n",
    "    import re\n",
    "    keep_list = [re.sub(r'#', '', line.strip()) for line in f.readlines() if line.strip()]\n",
    "\n",
    "# If it has # then it's a supertopic (has subtopics under it)\n",
    "with open('surahTopics.md') as f:\n",
    "    supertopic_list = [re.sub(r'#', '', line.strip()) for line in f.readlines() if (line.strip() and \"#\" in line)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to scrape all \\<li\\> elements and keep only those found in surahTopics.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved as JSON.\n"
     ]
    }
   ],
   "source": [
    "def extract_li_elements(url):\n",
    "    data = []  # List to store extracted data\n",
    "    try:\n",
    "        # Fetch the HTML content from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception if the request fails\n",
    "        html_content = response.content\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all <li> elements within the <ul>\n",
    "        li_elements = soup.find_all('li')\n",
    "\n",
    "        # Extract and store the text content and link (if available) of each <li> element\n",
    "        for li in li_elements:\n",
    "            link = li.find('a', href=\"javascript:;\")\n",
    "            if link:\n",
    "                sibling_link = link.find_next_sibling('a')\n",
    "                if sibling_link:\n",
    "                    li_text = sibling_link.get_text(strip=True)\n",
    "                    li_link = sibling_link.get('href')\n",
    "                else:\n",
    "                    li_text = link.get_text(strip=True)\n",
    "                    li_link = link.get('href')\n",
    "            else:\n",
    "                link = li.find('a')\n",
    "                if link:\n",
    "                    li_text = link.get_text(strip=True)\n",
    "                    li_link = link.get('href')\n",
    "                else:\n",
    "                    li_text = None\n",
    "                    li_link = None\n",
    "                    continue\n",
    "                \n",
    "            if li_text not in keep_list:\n",
    "                continue\n",
    "            data.append({\"text\":li_text, \"link\": li_link})\n",
    "\n",
    "        # Save the data as JSON\n",
    "        with open('surahTopicsFlat.json', 'w') as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(\"Data has been saved as JSON.\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching content from {url}: {e}\")\n",
    "        \n",
    "# Replace the URL below with the actual website you want to scrape\n",
    "target_url = \"https://shamela.ws/book/22915\"\n",
    "extract_li_elements(target_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Assumption for Ayah Range for Topic: Numbers within () in the longest {...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def get_longest_text_in_braces_with_parenthesis(link):\n",
    "    # Fetch HTML content\n",
    "    response = requests.get(link)\n",
    "    html_text = response.text\n",
    "\n",
    "    # Define regex pattern\n",
    "    pattern = r'\\{([^{}]+)\\}'\n",
    "    \n",
    "    # Find all matches\n",
    "    matches = re.findall(pattern, html_text)\n",
    "    \n",
    "    # Find the longest match\n",
    "    longest_match = max(matches, key=len)\n",
    "    \n",
    "    # Find items in parenthesis in the longest match\n",
    "    parenthesis_items = re.findall(r'\\((.*?)\\)', longest_match)\n",
    "    \n",
    "    return parenthesis_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Ayah Ranges by Calling Function Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8409f754f1f4d5db867508e8eff923a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# read extracted_data.json\n",
    "with open('surahTopicsFlat.json') as f:\n",
    "    extracted_data = json.load(f)\n",
    "\n",
    "# Define the total number of iterations for tqdm\n",
    "total_iterations = len(extracted_data)\n",
    "\n",
    "# Iterate through each item in extracted_data\n",
    "for item in tqdm(extracted_data, total=total_iterations, desc=\"Processing\"):\n",
    "    # Modify each item\n",
    "    item[\"Ayahs\"] = get_longest_text_in_braces_with_parenthesis(item[\"link\"])\n",
    "    # Special case where it fails\n",
    "    if \"نعم الله العظمى على عباده\" in item[\"text\"]:\n",
    "        item[\"Ayahs\"] = [\"٧٨\", \"٧٩\", \"٨٠\"]\n",
    "    # Supertopics replicate Ayahs for the first subtopic (we don't want that, they cover all subsequent topics starting with a number)\n",
    "    if item[\"text\"] in supertopic_list:\n",
    "        item[\"Ayahs\"] = []\n",
    "        item[\"supertopic\"] = True\n",
    "\n",
    "# save modified extracted_data.json\n",
    "with open('surahTopicsFlat.json', 'w') as f:\n",
    "    json.dump(extracted_data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regroup Topics: Those Belonging to Same Surah → Same Object in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# read extracted_data_ann.json\n",
    "with open('surahTopicsFlat.json') as f:\n",
    "    extracted_data = json.load(f)\n",
    "\n",
    "# find the inds where obj.test has word سورة\n",
    "inds = []\n",
    "for index, obj in enumerate(extracted_data):\n",
    "    # Want to extract only real 'سورة' while handling that it can also appear as a topic\n",
    "    if 'سورة' in obj['text'] and (len(obj['text'].split(' ')) < 6 or index == 0) or 'الانشراح' in obj['text']:\n",
    "        inds.append(index)\n",
    "\n",
    "# between each two inds and inds, group the items in an array with key being the surah at the ind\n",
    "# Iterate over pairs of inds\n",
    "grouped_data = {}\n",
    "\n",
    "for i in range(len(inds) - 1):\n",
    "    start_index = inds[i]\n",
    "    end_index = inds[i + 1] \n",
    "    \n",
    "    # Extract surah name from start index\n",
    "    surah_name = extracted_data[start_index]['text']\n",
    "    if \"ميزة سورة\" in surah_name:                       \n",
    "        continue\n",
    "    # Group items between start and end inds\n",
    "    group = [obj for obj in extracted_data[start_index:end_index]]\n",
    "    \n",
    "    # Add group to the dictionary with surah name as key\n",
    "    grouped_data[surah_name] = group\n",
    "\n",
    "# Handle the last group (from the last index to the end)\n",
    "surah_name = extracted_data[inds[-1]]['text']\n",
    "last_group = [obj for obj in extracted_data[inds[-1]:]]\n",
    "grouped_data[surah_name] = last_group\n",
    "# save grouped data as surah_topics.json\n",
    "with open('surahTopics.json', 'w') as f:\n",
    "    json.dump(grouped_data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: سورة الفاتحة مكية وآياتها سبع نزلت بعد المدثر\n",
      "2: سورة البقرة\n",
      "3: سورة آل عمران\n",
      "4: سورة النساء\n",
      "5: سورة المائدة\n",
      "6: سورة الأنعام\n",
      "7: سورة الأعراف\n",
      "8: سورة الأنفال\n",
      "9: سورة التوبة\n",
      "10: سورة يونس عليه السلام\n",
      "11: سورة هود عليه السلام\n",
      "12: سورة يوسف عليه السلام\n",
      "13: سورة الرعد\n",
      "14: سورة إبراهيم عليه السلام\n",
      "15: سورة الحجر\n",
      "16: سورة النحل\n",
      "17: سورة الإسراء\n",
      "18: سورة الكهف\n",
      "19: سورة مريم\n",
      "20: سورة طه\n",
      "21: سورة الأنبياء\n",
      "22: سورة الحج\n",
      "23: سورة المؤمنون\n",
      "24: سورة النور\n",
      "25: سورة الفرقان\n",
      "26: سورة الشعراء\n",
      "27: سورة النمل\n",
      "28: سورة القصص\n",
      "29: سورة العنكبوت\n",
      "30: سورة الروم\n",
      "31: سورة لقمان\n",
      "32: سورة السجدة\n",
      "33: سورة الأحزاب\n",
      "34: سورة سبأ\n",
      "35: سورة فاطر\n",
      "36: سورة يس\n",
      "37: سورة الصافات\n",
      "38: سورة ص\n",
      "39: سورة الزمر\n",
      "40: سورة غافر أو: المؤمن\n",
      "41: سورة فصلت أو: السجدة\n",
      "42: سورة الشورى\n",
      "43: سورة الزخرف\n",
      "44: سورة الدخان\n",
      "45: سورة الجاثية\n",
      "46: سورة الأحقاف\n",
      "47: سورة محمد عليه الصلاة والسلام\n",
      "48: سورة الفتح\n",
      "49: سورة الحجرات\n",
      "50: سورة ق\n",
      "51: سورة الذاريات\n",
      "52: سورة الطور\n",
      "53: سورة النجم\n",
      "54: سورة القمر\n",
      "55: سورة الرحمن جل ذكره\n",
      "56: سورة الواقعة\n",
      "57: سورة الحديد\n",
      "58: سورة المجادلة\n",
      "59: سورة الحشر\n",
      "60: سورة الممتحنة\n",
      "61: سورة الصف\n",
      "62: سورة الجمعة\n",
      "63: سورة المنافقون\n",
      "64: سورة التغابن\n",
      "65: سورة الطلاق\n",
      "66: سورة التحريم\n",
      "67: سورة الملك، أو: تبارك\n",
      "68: سورة القلم\n",
      "69: سورة الحاقة\n",
      "70: سورة المعارج\n",
      "71: سورة نوح عليه السلام\n",
      "72: سورة الجن\n",
      "73: سورة المزمل\n",
      "74: سورة المدثر\n",
      "75: سورة القيامة\n",
      "76: سورة الإنسان، أو: الدهر\n",
      "77: سورة المرسلات\n",
      "78: سورة النبأ، أو: عم\n",
      "79: سورة النازعات\n",
      "80: سورة عبس\n",
      "81: سورة التكوير\n",
      "82: سورة الانفطار\n",
      "83: سورة المطففين\n",
      "84: سورة الانشقاق\n",
      "85: سورة البروج\n",
      "86: سورة الطارق\n",
      "87: سورة الأعلى\n",
      "88: سورة الغاشية\n",
      "89: سورة الفجر\n",
      "90: سورة البلد\n",
      "91: سورة الشمس\n",
      "92: سورة الليل\n",
      "93: سورة الضحى\n",
      "94: الشرح، أو: الانشراح\n",
      "95: سورة التين\n",
      "96: سورة العلق\n",
      "97: سورة القدر\n",
      "98: سورة البينة\n",
      "99: سورة الزلزلة\n",
      "100: سورة العاديات\n",
      "101: سورة القارعة\n",
      "102: سورة التكاثر\n",
      "103: سورة العصر\n",
      "104: سورة الهمزة\n",
      "105: سورة الفيل\n",
      "106: سورة قريش\n",
      "107: سورة الماعون\n",
      "108: سورة الكوثر\n",
      "109: سورة الكافرون\n",
      "110: سورة النصر\n",
      "111: سورة المسد، أو: اللهب\n",
      "112: سورة الإخلاص\n",
      "113: سورة الفلق\n",
      "114: سورة الناس\n"
     ]
    }
   ],
   "source": [
    "# print keys of grouped_data and index of each\n",
    "for index, (key, value) in enumerate(grouped_data.items()):\n",
    "    print(f\"{index+1}: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure Scraping is Correct:\n",
    "\n",
    "- Topics from each Surah have topics that cover all it's Ayahs \n",
    "\n",
    "- No gaps or repetitions of Ayahs across topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an array like 1, 2, 3, 4, 6, 7, 8 return the index where contiguity broke (e.g., i=4 here)\n",
    "def find_discontinuity_indices(arr):\n",
    "    discontinuity_indices = []\n",
    "    for i in range(1, len(arr)):\n",
    "        # Convert Arabic numerals to integers for comparison\n",
    "        current_num = int(arr[i])\n",
    "        prev_num = int(arr[i - 1])\n",
    "        \n",
    "        if current_num != prev_num + 1:\n",
    "            discontinuity_indices.append(i)\n",
    "    \n",
    "    return discontinuity_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 200 ayahs, but got 197 for سورة آل عمران at 2\n",
      "There is a gap in ayahs for سورة آل عمران at index 41 where ٤١ is followed by ٤٥\n",
      "\n",
      "\n",
      "Expected 98 ayahs, but got 95 for سورة مريم at 18\n",
      "There is a gap in ayahs for سورة مريم at index 50 where ٥٠ is followed by ٥٤\n",
      "\n",
      "\n",
      "Expected 64 ayahs, but got 0 for سورة النور at 23\n",
      "Expected 83 ayahs, but got 81 for سورة يس at 35\n",
      "There is a gap in ayahs for سورة يس at index 66 where ٦٦ is followed by ٦٩\n",
      "\n",
      "\n",
      "Expected 37 ayahs, but got 34 for سورة الجاثية at 44\n",
      "There is a gap in ayahs for سورة الجاثية at index 17 where ١٧ is followed by ٢١\n",
      "\n",
      "\n",
      "Expected 8 ayahs, but got 0 for سورة التين at 94\n",
      "Expected 3 ayahs, but got 0 for سورة الكوثر at 107\n"
     ]
    }
   ],
   "source": [
    "# for each key in grouped_data, loop on all objects and concatenate their obj.Ayahs\n",
    "\n",
    "# read surasList.json\n",
    "with open('surasList.json') as f:\n",
    "    suras_list = json.load(f)\n",
    "\n",
    "\n",
    "for i, (key, value) in enumerate(grouped_data.items()):\n",
    "    ayah_seq = []\n",
    "    for topic in value:\n",
    "        ayah_seq += topic['Ayahs']\n",
    "    \n",
    "    # First assumption mentioned above\n",
    "    if int(suras_list[i]['numAyas']) != len(ayah_seq):\n",
    "        print(f\"Expected {suras_list[i]['numAyas']} ayahs, but got {len(ayah_seq)} for {key} at {i}\")\n",
    "    \n",
    "    discs = find_discontinuity_indices(ayah_seq)\n",
    "    # Second assumption mentioned above\n",
    "    if len(discs) > 0:\n",
    "        for disc in discs:\n",
    "            print(f\"There is a gap in ayahs for {key} at index {disc} where {ayah_seq[disc-1]} is followed by {ayah_seq[disc]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually handling those errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No need to worry about the last two (they are short Suras with no topics provided by book)\n",
    "\n",
    "- Missing Ayah's in one of Surah Gatheya topics because Ayahs spanned two pages: missing ayahs added manually.\n",
    "\n",
    "- Surah Al-Noor was regenerated manually by making a surahTopicNoor.md (likely above in the scraping code but repeating the scraping is tedious)\n",
    "\n",
    "- Same for the topic in Surah Yassin as Gatheya (fixed)\n",
    "\n",
    "- Surah Maryem had a false positive for the super topic because its name was equivalent to one supertopic in another surah (fixed)\n",
    "\n",
    "- Surah Al-Omran same problem (fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check fixes are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 8 ayahs, but got 0 for سورة التين at 94\n",
      "Expected 3 ayahs, but got 0 for سورة الكوثر at 107\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('surasList.json') as f:\n",
    "    suras_list = json.load(f)\n",
    "    \n",
    "with open('surahSections.json') as f:               # This is where fixed went to avoid being later overwritten\n",
    "    grouped_data = json.load(f)\n",
    "\n",
    "for i, (key, value) in enumerate(grouped_data.items()):\n",
    "    ayah_seq = []\n",
    "    for topic in value:\n",
    "        ayah_seq += topic['Ayahs']\n",
    "    \n",
    "    # First assumption mentioned above\n",
    "    if int(suras_list[i]['numAyas']) != len(ayah_seq):\n",
    "        print(f\"Expected {suras_list[i]['numAyas']} ayahs, but got {len(ayah_seq)} for {key} at {i}\")\n",
    "    \n",
    "    discs = find_discontinuity_indices(ayah_seq)\n",
    "    # Second assumption mentioned above\n",
    "    if len(discs) > 0:\n",
    "        for disc in discs:\n",
    "            print(f\"There is a gap in ayahs for {key} at index {disc} where {ayah_seq[disc-1]} is followed by {ayah_seq[disc]}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect\n",
    "\n",
    "الحمدلله"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
